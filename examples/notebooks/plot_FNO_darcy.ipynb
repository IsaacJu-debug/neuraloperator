{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a TFNO on Darcy-Flow\n",
    "\n",
    "In this example, we demonstrate how to use the small Darcy-Flow example we ship with the package\n",
    "to train a Tensorized Fourier-Neural Operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from neuralop.models import TFNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.datasets import load_darcy_flow_small\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Darcy flow dataset in 128x128 resolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test db at resolution 32 with 50 samples and batch-size=32\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loaders, data_processor = load_darcy_flow_small(\n",
    "        n_train=1000, batch_size=32, \n",
    "        test_resolutions=[16, 32], n_tests=[100, 50],\n",
    "        test_batch_sizes=[32, 32],\n",
    "        positional_encoding=True\n",
    ")\n",
    "data_processor = data_processor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a tensorized FNO model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our model has 523257 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = TFNO(n_modes=(16, 16), hidden_channels=32, projection_channels=64, factorization='tucker', rank=0.42)\n",
    "model = model.to(device)\n",
    "\n",
    "n_params = count_model_params(model)\n",
    "print(f'\\nOur model has {n_params} parameters.')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                lr=8e-3, \n",
    "                                weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses={'h1': h1loss, 'l2': l2loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### MODEL ###\n",
      " TFNO(\n",
      "  (fno_blocks): FNOBlocks(\n",
      "    (convs): SpectralConv(\n",
      "      (weight): ModuleList(\n",
      "        (0-3): 4 x ComplexTuckerTensor(shape=(32, 32, 16, 9), rank=(26, 26, 13, 7))\n",
      "      )\n",
      "    )\n",
      "    (fno_skips): ModuleList(\n",
      "      (0-3): 4 x Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (lifting): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (projection): MLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "### OPTIMIZER ###\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.008\n",
      "    lr: 0.008\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "\n",
      "### SCHEDULER ###\n",
      " <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7fce6328b670>\n",
      "\n",
      "### LOSSES ###\n",
      "\n",
      " * Train: <neuralop.losses.data_losses.H1Loss object at 0x7fce6328ba30>\n",
      "\n",
      " * Test: {'h1': <neuralop.losses.data_losses.H1Loss object at 0x7fce6328ba30>, 'l2': <neuralop.losses.data_losses.LpLoss object at 0x7fcd72f2a980>}\n"
     ]
    }
   ],
   "source": [
    "print('\\n### MODEL ###\\n', model)\n",
    "print('\\n### OPTIMIZER ###\\n', optimizer)\n",
    "print('\\n### SCHEDULER ###\\n', scheduler)\n",
    "print('\\n### LOSSES ###')\n",
    "print(f'\\n * Train: {train_loss}')\n",
    "print(f'\\n * Test: {eval_losses}')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.override_load_to_device=False\n",
      "self.overrides_loss=False\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, n_epochs=20,\n",
    "                  device=device,\n",
    "                  data_processor=data_processor,\n",
    "                  wandb_log=False,\n",
    "                  log_test_interval=3,\n",
    "                  use_distributed=False,\n",
    "                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually train the model on our small Darcy-Flow dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'32_h1': 0.2961893081665039, '32_l2': 0.21333395957946777}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(train_loader=train_loader,\n",
    "              test_loaders=test_loaders,\n",
    "              optimizer=optimizer,\n",
    "              scheduler=scheduler, \n",
    "              regularizer=False, \n",
    "              training_loss=train_loss,\n",
    "              eval_losses=eval_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prediction, and compare with the ground-truth \n",
    "Note that we trained on a very small resolution for\n",
    "a very small number of epochs\n",
    "In practice, we would train at larger resolution, on many more samples.\n",
    "\n",
    "However, for practicity, we created a minimal example that\n",
    "i) fits in just a few Mb of memory\n",
    "ii) can be trained quickly on CPU\n",
    "\n",
    "In practice we would train a Neural Operator on one or multiple GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     14\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, index\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     17\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput x\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5759\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5759\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5760\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5762\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/matplotlib/image.py:723\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    722\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/matplotlib/image.py:686\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_normalize_image_array\u001b[39m(A):\n\u001b[1;32m    682\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m    Image subclasses.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/matplotlib/cbook.py:733\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_masked_invalid\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 733\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39misnative:\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mbyteswap(inplace\u001b[38;5;241m=\u001b[39mcopy)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnewbyteorder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/home/groups/tchelepi/ju1/02_dl_modeling/00_python_env/ai_science_gnn/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = test_loaders[32].dataset\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "for index in range(3):\n",
    "    data = test_samples[index]\n",
    "    data = data_processor.preprocess(data, batched=False)\n",
    "    # Input x\n",
    "    x = data['x']\n",
    "    # Ground-truth\n",
    "    y = data['y']\n",
    "    # Model prediction\n",
    "    out = model(x.unsqueeze(0))\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 1)\n",
    "    ax.imshow(x[0], cmap='gray')\n",
    "    if index == 0: \n",
    "        ax.set_title('Input x')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 2)\n",
    "    ax.imshow(y.squeeze())\n",
    "    if index == 0: \n",
    "        ax.set_title('Ground-truth y')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, index*3 + 3)\n",
    "    ax.imshow(out.squeeze().detach().numpy())\n",
    "    if index == 0: \n",
    "        ax.set_title('Model prediction')\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "fig.suptitle('Inputs, ground-truth output and prediction.', y=0.98)\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
